LORA_CONFIG_PARAMS:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    bias: "none"
    task_type: "CAUSAL_LM"

TRAINING_ARGS_PARAMS:
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 8
    warmup_steps: 100
    max_steps: 50
    learning_rate: 2e-5
    fp16: true
    logging_steps: 1
    output_dir: "outputs"